# Main Logstash Pipeline for PM AI MVP

input {
  # Beats input (from Filebeat)
  beats {
    port => 5044
    type => "beats"
  }

  # TCP input for direct log forwarding
  tcp {
    port => 5000
    type => "tcp"
    codec => json_lines
  }

  # HTTP input for application logs
  http {
    port => 8080
    type => "http"
    codec => json
  }

  # Docker logs input
  docker {
    type => "docker"
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed"
    }
  }

  # Parse application logs
  if [type] == "application" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:level}\] %{DATA:logger} - %{GREEDYDATA:log_message}" 
      }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }

  # Parse Nginx logs
  if [type] == "nginx" {
    grok {
      match => { 
        "message" => "%{NGINXACCESS}" 
      }
    }
    
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }
    
    geoip {
      source => "clientip"
      target => "geoip"
    }
    
    useragent {
      source => "agent"
      target => "user_agent"
    }
  }

  # Parse PostgreSQL logs
  if [type] == "postgresql" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:level}\] %{GREEDYDATA:log_message}" 
      }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }

  # Parse Redis logs
  if [type] == "redis" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{DATA:level} %{GREEDYDATA:log_message}" 
      }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }

  # Parse Docker logs
  if [type] == "docker" {
    json {
      source => "message"
    }
    
    if [stream] == "stderr" {
      mutate {
        add_tag => [ "error" ]
      }
    }
  }

  # Parse FastAPI/Backend logs
  if [type] == "backend" or [service] == "backend" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:level}\] %{DATA:logger} - %{GREEDYDATA:log_message}" 
      }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
    
    # Extract request information
    if [log_message] =~ /Request/ {
      grok {
        match => { 
          "log_message" => "Request %{DATA:method} %{DATA:path} - %{DATA:status_code} - %{NUMBER:response_time:float}ms" 
        }
      }
    }
  }

  # Parse Frontend logs
  if [type] == "frontend" or [service] == "frontend" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:level}\] %{DATA:logger} - %{GREEDYDATA:log_message}" 
      }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }

  # Add common fields
  mutate {
    add_field => { 
      "environment" => "%{ENVIRONMENT}"
      "service_name" => "%{SERVICE_NAME}"
      "version" => "%{VERSION}"
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => [ "host", "agent", "ecs", "input", "log" ]
  }

  # Add error classification
  if [level] == "ERROR" or [level] == "FATAL" or [level] == "CRITICAL" {
    mutate {
      add_tag => [ "error" ]
    }
  }

  if [level] == "WARN" or [level] == "WARNING" {
    mutate {
      add_tag => [ "warning" ]
    }
  }

  # Add performance metrics
  if [response_time] {
    mutate {
      add_tag => [ "performance" ]
    }
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "pm-ai-logs-%{+YYYY.MM.dd}"
    template_name => "pm-ai-logs"
    template => "/usr/share/logstash/templates/pm-ai-logs.json"
    template_overwrite => true
  }

  # Output to stdout for debugging
  stdout {
    codec => rubydebug
  }
}
